{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_polygon_on_image(points, image, value):\n",
    "  points = [(x, y) for x, y in points]\n",
    "  draw = ImageDraw.Draw(image, 'RGB')\n",
    "  draw.polygon(points, fill=(value, value, value))\n",
    "  return image\n",
    "\n",
    "def generate_ground_truth(ground_truth, image):\n",
    "  black_image = Image.new('RGB', image.size)\n",
    "  for shape in ground_truth['shapes']:\n",
    "    points = shape['points']\n",
    "    image = draw_polygon_on_image(points, black_image, shape['group_id'])\n",
    "  return image\n",
    "\n",
    "def process_images():\n",
    "  path = r'C:\\Users\\topra\\Documents\\Jupyter\\Satellite-Model\\ViT\\data'\n",
    "  path = os.path.join(path, 'labels', '*.json')\n",
    "  image_names_cand = glob.glob(path)\n",
    "  image_names = []\n",
    "  label_names_cand = [image_name.replace('images', 'labels').replace('.png', '.json') for image_name in image_names_cand]\n",
    "  label_names = []\n",
    "  for label_name, i in zip(label_names_cand, range(len(label_names_cand))):\n",
    "    if os.path.exists(label_name):\n",
    "      image_name = label_name.replace('labels', 'images').replace('.json', '.png')\n",
    "      image_names.append(image_name)\n",
    "      label_names.append(label_name)\n",
    "  for image_name, label_name in zip(image_names, label_names):\n",
    "    image = Image.open(image_name)\n",
    "    with open(label_name, 'r') as file:\n",
    "      ground_truth = json.load(file)\n",
    "      ground_truth_image = generate_ground_truth(ground_truth, image)\n",
    "      ground_truth_image.save(image_name.replace('images', 'truth'))\n",
    "\n",
    "process_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "  def __init__(self, image_folder, truth_folder, transform_data=None, transform_truth=None):\n",
    "    path = r'C:\\Users\\topra\\Documents\\Jupyter\\Satellite-Model\\ViT\\data'\n",
    "    image_path = os.path.join(path, image_folder, '*.png')\n",
    "    truth_path = os.path.join(path, truth_folder, '*.png')\n",
    "    image_names_cand = glob.glob(image_path)\n",
    "    truth_images_cand = glob.glob(truth_path)\n",
    "    self.image_names = []\n",
    "    self.truth_images = []\n",
    "    for image_name, i in zip(image_names_cand, range(len(image_names_cand))):\n",
    "      truth_name = image_name.replace('images', 'truth')\n",
    "      if truth_name in truth_images_cand:\n",
    "        self.image_names.append(image_name)\n",
    "        self.truth_images.append(truth_name)\n",
    "    self.transform_data = transform_data\n",
    "    self.transform_truth = transform_truth\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.image_names)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    image = Image.open(self.image_names[idx])\n",
    "    truth = Image.open(self.truth_images[idx])\n",
    "    if self.transform_data:\n",
    "      image = self.transform_data(image)\n",
    "    if self.transform_truth:\n",
    "      truth = self.transform_truth(truth)\n",
    "    return image, truth\n",
    "\n",
    "transform_data = transforms.Compose([\n",
    "  transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_truth = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Grayscale()\n",
    "])\n",
    "\n",
    "dataset = SatelliteDataset('images', 'truth', transform_data=transform_data, transform_truth=transform_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchPositionalEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 channels: int=3,\n",
    "                 patch_size: int=128,\n",
    "                 max_len: int=512):\n",
    "        super(PatchPositionalEmbedding, self).__init__()\n",
    "\n",
    "        # Layers\n",
    "        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size, padding=0)       # (batch, C*patch_size*patch_size, num_tokens)\n",
    "        self.embed = nn.Parameter(torch.randn(1, max_len, channels*patch_size*patch_size))  # (batch, max_len, C*patch_size*patch_size)\n",
    "        self.pred = torch.zeros(1, 1, channels*patch_size*patch_size)                       # (batch, max_len, C*patch_size*patch_size)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): (batch, C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (batch, min(max_len, num_tokens), C*patch_size*patch_size)\n",
    "        \"\"\"\n",
    "        x = self.unfold(x)\n",
    "        x = torch.concat([x.transpose(1, 2), self.pred], dim=1)\n",
    "        return x + self.embed[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                channels: int=3,\n",
    "                patch_size: int=128):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self,\n",
    "                v: torch.Tensor,\n",
    "                k: torch.Tensor,\n",
    "                q: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.dot(q, k)\n",
    "        x = x / (q.size(-1) ** 0.5)\n",
    "        x = self.softmax(x)\n",
    "        x = torch.matmul(x, v)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 channels: int=3,\n",
    "                 patch_size: int=128,\n",
    "                 h: int=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.linear_v = [nn.Linear(channels*patch_size*patch_size, channels*patch_size*patch_size*h) for _ in range(h)]\n",
    "        self.linear_k = [nn.Linear(channels*patch_size*patch_size, channels*patch_size*patch_size*h) for _ in range(h)]\n",
    "        self.linear_q = [nn.Linear(channels*patch_size*patch_size, channels*patch_size*patch_size*h) for _ in range(h)]\n",
    "        self.scaled_dot = [ScaledDotProductAttention(channels=channels, patch_size=patch_size) for _ in range(h)]\n",
    "        self.linear_out = nn.Linear(channels*patch_size*patch_size*h, channels*patch_size*patch_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = []\n",
    "        for i in range(h):\n",
    "            v = self.linear_v[i](x)\n",
    "            k = self.linear_k[i](x)\n",
    "            q = self.linear_q[i](x)\n",
    "            y.append(self.scaled_dot[i](v, k, q))\n",
    "        x = torch.concat(y, dim=-1)\n",
    "        x = self.linear_out(x)\n",
    "        return x\n",
    "\n",
    "class TransformEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 channels: int=3,\n",
    "                 patch_size: int=128,\n",
    "                 max_len: int=512):\n",
    "        super(TransformEncoder, self).__init__()\n",
    "        self.norm_1 = nn.LayerNorm(channels*patch_size*patch_size)\n",
    "        self.attention = MultiHeadAttention()\n",
    "        self.norm_2 = nn.LayerNorm(channels*patch_size*patch_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(channels*patch_size*patch_size, channels*patch_size*patch_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(channels*patch_size*patch_size, channels*patch_size*patch_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(channels*patch_size*patch_size, channels*patch_size*patch_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(channels*patch_size*patch_size, channels*patch_size*patch_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(channels*patch_size*patch_size, channels*patch_size*patch_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): (batch, num_tokens, C*patch_size*patch_size)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (batch, num_tokens, C*patch_size*patch_size)\n",
    "        \"\"\"\n",
    "        x_2 = self.norm_1(x)\n",
    "        x_2 = self.attention(x_2)\n",
    "        x_2 = x_2 + x\n",
    "        x = x_2\n",
    "        x_2 = self.norm_2(x)\n",
    "        x_2 = self.mlp(x_2)\n",
    "        x_2 = x_2 + x\n",
    "        return x_2\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 channels: int=3,\n",
    "                 patch_size: int=128,\n",
    "                 max_len: int=512,\n",
    "                 num_layers: int=12):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embed = PatchPositionalEmbedding(channels=channels, patch_size=patch_size, max_len=max_len)\n",
    "        self.layers = nn.Sequential(*[TransformEncoder(channels=channels, patch_size=patch_size, max_len=max_len) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.layers(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2048, 2048]) -> torch.Size([1, 257, 49152])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "image, truth = dataset[0] # Get the first image and its ground truth\n",
    "# enlarge the images to 2048x2048\n",
    "image = transforms.Resize((2048, 2048))(image)\n",
    "truth = transforms.Resize((2048, 2048))(truth)\n",
    "\n",
    "patch_embedding = PatchPositionalEmbedding(channels=3, patch_size=128, max_len=1024)\n",
    "x = patch_embedding(image.unsqueeze(0))\n",
    "print(f\"{image.shape} -> {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "EMBEDDING_SIZE = 2048\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.embed = PatchPositionalEmbedding(channels=3, patch_size=128, max_len=1024)\n",
    "    self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=EMBEDDING_SIZE, nhead=8), num_layers=6)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.vit(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
